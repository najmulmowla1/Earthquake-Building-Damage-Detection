# -*- coding: utf-8 -*-
"""Vision Transformer Block.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14QDiYAGln_KzZIk3I_-fLzna3kq2ndRy
"""

# vit.py

import tensorflow as tf
from tensorflow.keras import layers
from attention import SCA_HMDA  # Import your custom attention module

class VisionTransformer(layers.Layer):
    def __init__(self, patch_size, num_patches, embed_dim, num_heads, num_layers, **kwargs):
        super(VisionTransformer, self).__init__(**kwargs)
        self.patch_size = patch_size
        self.num_patches = num_patches
        self.embed_dim = embed_dim
        self.flatten_patches = layers.Reshape((num_patches, -1))
        self.projection = layers.Dense(embed_dim)
        self.positional_encoding = layers.Embedding(input_dim=num_patches, output_dim=embed_dim)
        self.sca_hmda_blocks = [SCA_HMDA(num_heads, embed_dim) for _ in range(num_layers)]

    def build(self, input_shape):
        self.num_patches = (input_shape[1] // self.patch_size) * (input_shape[2] // self.patch_size)
        super(VisionTransformer, self).build(input_shape)

    def call(self, inputs):
        patches = tf.image.extract_patches(
            images=inputs,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding="VALID",
        )
        flattened_patches = self.flatten_patches(patches)
        embeddings = self.projection(flattened_patches)
        embeddings += self.positional_encoding(tf.range(tf.shape(embeddings)[1]))

        for sca_hmda_block in self.sca_hmda_blocks:
            embeddings = sca_hmda_block(embeddings)

        return tf.reduce_mean(embeddings, axis=1)